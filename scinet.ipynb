{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scinet import SciNet\n",
    "import pandas as pd\n",
    "from scinet_utils import target_loss \n",
    "from loader import build_dataloader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scinet import SciNet\n",
    "from scinet_utils import target_loss\n",
    "from loader import build_dataloader\n",
    "\n",
    "def generate_data(size, size_, t_max):\n",
    "    t = np.linspace(0, t_max, size)\n",
    "    min_fr, max_fr = 0.01, 100\n",
    "    fr = np.linspace(min_fr, max_fr, size_)\n",
    "    start_st, end_st = 0.01, 100\n",
    "    st = np.logspace(np.log10(start_st), np.log10(end_st), size_, endpoint=True)\n",
    "\n",
    "    def f(t, st, fr):\n",
    "        return st**2 * fr * (1 - t/st - np.exp(-t/st))\n",
    "\n",
    "    data = []\n",
    "    for st_ in st:\n",
    "        for fr_ in fr:\n",
    "            example = list(f(t, st_, fr_))\n",
    "            t_pred = np.random.uniform(0, t_max)\n",
    "            pred = f(t_pred, st_, fr_)\n",
    "            example.extend([fr_, st_, t_pred, pred])\n",
    "            data.append(example)\n",
    "\n",
    "    columns = [str(i) for i in range(size)]\n",
    "    columns.extend([\"fr\", \"st\", \"t_pred\", \"pred\"])\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def train_sci_net(scinet, dataloader, optimizer, scheduler, beta, N_EPOCHS, device):\n",
    "    hist_error = []\n",
    "    hist_kl = []\n",
    "    hist_loss = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_error = []\n",
    "        epoch_kl = []\n",
    "        epoch_loss = []\n",
    "        for minibatch in dataloader:\n",
    "            time_series, fr, st, question, answer = (\n",
    "                minibatch['time_series'].to(device) / 5,\n",
    "                minibatch['fr'].to(device) / 5,\n",
    "                minibatch['st'].to(device) / 5,\n",
    "                minibatch['question'].to(device) / 5,\n",
    "                minibatch['answer'].to(device) / 5\n",
    "            )\n",
    "            inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "            outputs = answer\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = scinet.forward(inputs)\n",
    "            loss_ = target_loss(pred, outputs)\n",
    "            kl = beta * scinet.kl_loss\n",
    "            loss = loss_ + kl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "            epoch_error.append(float(error))\n",
    "            epoch_kl.append(float(kl.data.detach().cpu().numpy()))\n",
    "            epoch_loss.append(float(loss_.data.detach().cpu().numpy()))\n",
    "\n",
    "        hist_error.append(np.mean(epoch_error))\n",
    "        hist_loss.append(np.mean(epoch_loss))\n",
    "        hist_kl.append(np.mean(epoch_kl))\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "        print(\"Epoch %d -- loss %f, RMS error %f, KL %f\" % (epoch+1, hist_loss[-1], hist_error[-1], hist_kl[-1]))\n",
    "\n",
    "    return hist_error, hist_kl, hist_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: SGD lr 0.001000 -> 0.000980\n",
      "Epoch 1 -- loss 87192.357004, RMS error 14.192720, KL 30315.707145\n",
      "Epoch 2: SGD lr 0.000980 -> 0.000960\n",
      "Epoch 2 -- loss 71414.306790, RMS error 12.934764, KL 899.540144\n",
      "Epoch 3: SGD lr 0.000960 -> 0.000941\n",
      "Epoch 3 -- loss 29297.325136, RMS error 7.641603, KL 411.668688\n",
      "Epoch 4: SGD lr 0.000941 -> 0.000921\n",
      "Epoch 4 -- loss 6621.528093, RMS error 3.508350, KL 331.298824\n",
      "Epoch 5: SGD lr 0.000921 -> 0.000901\n",
      "Epoch 5 -- loss 3076.450972, RMS error 2.311101, KL 271.402239\n",
      "Epoch 6: SGD lr 0.000901 -> 0.000881\n",
      "Epoch 6 -- loss 1771.622371, RMS error 1.761615, KL 221.288146\n",
      "Epoch 7: SGD lr 0.000881 -> 0.000861\n",
      "Epoch 7 -- loss 1119.256538, RMS error 1.412735, KL 191.921584\n",
      "Epoch 8: SGD lr 0.000861 -> 0.000841\n",
      "Epoch 8 -- loss 763.089544, RMS error 1.201446, KL 168.596913\n",
      "Epoch 9: SGD lr 0.000841 -> 0.000822\n",
      "Epoch 9 -- loss 545.230520, RMS error 1.038492, KL 152.727588\n",
      "Epoch 10: SGD lr 0.000822 -> 0.000802\n",
      "Epoch 10 -- loss 425.998118, RMS error 0.932242, KL 139.336499\n",
      "Epoch 11: SGD lr 0.000802 -> 0.000782\n",
      "Epoch 11 -- loss 381.214174, RMS error 0.882793, KL 129.973119\n",
      "Epoch 12: SGD lr 0.000782 -> 0.000762\n",
      "Epoch 12 -- loss 363.621013, RMS error 0.886774, KL 122.330622\n",
      "Epoch 13: SGD lr 0.000762 -> 0.000742\n",
      "Epoch 13 -- loss 223.903356, RMS error 0.694987, KL 114.073112\n",
      "Epoch 14: SGD lr 0.000742 -> 0.000723\n",
      "Epoch 14 -- loss 188.354957, RMS error 0.649905, KL 107.540692\n",
      "Epoch 15: SGD lr 0.000723 -> 0.000703\n",
      "Epoch 15 -- loss 159.579833, RMS error 0.599071, KL 102.254826\n",
      "Epoch 16: SGD lr 0.000703 -> 0.000683\n",
      "Epoch 16 -- loss 155.924882, RMS error 0.589866, KL 96.533379\n",
      "Epoch 17: SGD lr 0.000683 -> 0.000663\n",
      "Epoch 17 -- loss 153.120664, RMS error 0.574237, KL 91.698342\n",
      "Epoch 18: SGD lr 0.000663 -> 0.000643\n",
      "Epoch 18 -- loss 118.018116, RMS error 0.518025, KL 87.883435\n",
      "Epoch 19: SGD lr 0.000643 -> 0.000623\n",
      "Epoch 19 -- loss 103.613502, RMS error 0.482585, KL 84.453028\n",
      "Epoch 20: SGD lr 0.000623 -> 0.000604\n",
      "Epoch 20 -- loss 88.133372, RMS error 0.455050, KL 81.341628\n",
      "Epoch 21: SGD lr 0.000604 -> 0.000584\n",
      "Epoch 21 -- loss 80.212029, RMS error 0.439328, KL 77.793300\n",
      "Epoch 22: SGD lr 0.000584 -> 0.000564\n",
      "Epoch 22 -- loss 70.696940, RMS error 0.409188, KL 75.198327\n",
      "Epoch 23: SGD lr 0.000564 -> 0.000544\n",
      "Epoch 23 -- loss 196.538825, RMS error 0.647607, KL 74.833856\n",
      "Epoch 24: SGD lr 0.000544 -> 0.000524\n",
      "Epoch 24 -- loss 86.292584, RMS error 0.432884, KL 73.101330\n",
      "Epoch 25: SGD lr 0.000524 -> 0.000504\n",
      "Epoch 25 -- loss 67.303979, RMS error 0.388733, KL 70.655374\n",
      "Epoch 26: SGD lr 0.000504 -> 0.000485\n",
      "Epoch 26 -- loss 82.536920, RMS error 0.415772, KL 68.481820\n",
      "Epoch 27: SGD lr 0.000485 -> 0.000465\n",
      "Epoch 27 -- loss 51.953694, RMS error 0.340615, KL 66.217647\n",
      "Epoch 28: SGD lr 0.000465 -> 0.000445\n",
      "Epoch 28 -- loss 55.102506, RMS error 0.348145, KL 64.173358\n",
      "Epoch 29: SGD lr 0.000445 -> 0.000425\n",
      "Epoch 29 -- loss 78.736613, RMS error 0.417260, KL 63.800053\n",
      "Epoch 30: SGD lr 0.000425 -> 0.000405\n",
      "Epoch 30 -- loss 49.662842, RMS error 0.338099, KL 62.381746\n",
      "Epoch 31: SGD lr 0.000405 -> 0.000386\n",
      "Epoch 31 -- loss 58.214505, RMS error 0.346170, KL 61.003789\n",
      "Epoch 32: SGD lr 0.000386 -> 0.000366\n",
      "Epoch 32 -- loss 30.888996, RMS error 0.280203, KL 58.963453\n",
      "Epoch 33: SGD lr 0.000366 -> 0.000346\n",
      "Epoch 33 -- loss 40.902882, RMS error 0.314609, KL 57.590322\n",
      "Epoch 34: SGD lr 0.000346 -> 0.000326\n",
      "Epoch 34 -- loss 33.003545, RMS error 0.285457, KL 56.412154\n",
      "Epoch 35: SGD lr 0.000326 -> 0.000306\n",
      "Epoch 35 -- loss 28.625951, RMS error 0.265201, KL 55.249644\n",
      "Epoch 36: SGD lr 0.000306 -> 0.000286\n",
      "Epoch 36 -- loss 54.897576, RMS error 0.346859, KL 54.596445\n",
      "Epoch 37: SGD lr 0.000286 -> 0.000267\n",
      "Epoch 37 -- loss 42.516677, RMS error 0.301928, KL 54.289262\n",
      "Epoch 38: SGD lr 0.000267 -> 0.000247\n",
      "Epoch 38 -- loss 47.801169, RMS error 0.319379, KL 53.834383\n",
      "Epoch 39: SGD lr 0.000247 -> 0.000227\n",
      "Epoch 39 -- loss 21.027044, RMS error 0.231712, KL 52.968353\n",
      "Epoch 40: SGD lr 0.000227 -> 0.000207\n",
      "Epoch 40 -- loss 30.361630, RMS error 0.259784, KL 52.078605\n",
      "Epoch 41: SGD lr 0.000207 -> 0.000187\n",
      "Epoch 41 -- loss 22.778626, RMS error 0.236918, KL 51.341064\n",
      "Epoch 42: SGD lr 0.000187 -> 0.000168\n",
      "Epoch 42 -- loss 16.875946, RMS error 0.213270, KL 50.579829\n",
      "Epoch 43: SGD lr 0.000168 -> 0.000148\n",
      "Epoch 43 -- loss 19.972030, RMS error 0.227183, KL 49.970189\n",
      "Epoch 44: SGD lr 0.000148 -> 0.000128\n",
      "Epoch 44 -- loss 22.912277, RMS error 0.228411, KL 49.424140\n",
      "Epoch 45: SGD lr 0.000128 -> 0.000108\n",
      "Epoch 45 -- loss 17.964045, RMS error 0.208022, KL 49.027455\n",
      "Epoch 46: SGD lr 0.000108 -> 0.000088\n",
      "Epoch 46 -- loss 11.926560, RMS error 0.178277, KL 48.485860\n",
      "Epoch 47: SGD lr 0.000088 -> 0.000068\n",
      "Epoch 47 -- loss 12.612070, RMS error 0.180943, KL 48.019797\n",
      "Epoch 48: SGD lr 0.000068 -> 0.000049\n",
      "Epoch 48 -- loss 10.713747, RMS error 0.171993, KL 47.675084\n",
      "Epoch 49: SGD lr 0.000049 -> 0.000029\n",
      "Epoch 49 -- loss 9.174570, RMS error 0.159729, KL 47.336156\n",
      "Epoch 50: SGD lr 0.000029 -> 0.000009\n",
      "Epoch 50 -- loss 7.829222, RMS error 0.149550, KL 47.127420\n",
      "Model saved to saved_models/scinet1-25epoch50.dat\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sizes = [25]\n",
    "N_EPOCHS = 50\n",
    "size_ = 200\n",
    "t_max = 5\n",
    "data_file = \"data.csv\"\n",
    "for size in sizes: \n",
    "    df = generate_data(size, size_, t_max)\n",
    "    df.to_csv(data_file)\n",
    "\n",
    "    scinet = SciNet(size, 1, 3, 100).to(device)  # Move the model to the GPU\n",
    "    dataloader = build_dataloader(size=size, batch_size=128)\n",
    "\n",
    "    SAVE_PATH = f\"saved_models/scinet1-{size}epoch{N_EPOCHS}.dat\"\n",
    "    optimizer = optim.Adam(scinet.parameters(), lr=0.001)\n",
    "    beta = 0.5\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.009, total_iters=N_EPOCHS)\n",
    "\n",
    "    hist_error = []\n",
    "    hist_kl = []\n",
    "    hist_loss = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_error = []\n",
    "        epoch_kl = []\n",
    "        epoch_loss = []\n",
    "        for minibatch in dataloader:\n",
    "            time_series, fr, st, question, answer = (\n",
    "                minibatch['time_series'].to(device) / 5,\n",
    "                minibatch['fr'].to(device) / 5,\n",
    "                minibatch['st'].to(device) / 5,\n",
    "                minibatch['question'].to(device) / 5,\n",
    "                minibatch['answer'].to(device) / 5\n",
    "            )\n",
    "            inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "            outputs = answer\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input, pred = scinet.forward(inputs)\n",
    "            loss_ = target_loss(pred, outputs)\n",
    "            kl = beta * scinet.kl_loss\n",
    "            loss = loss_ + kl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "            epoch_error.append(float(error))\n",
    "            epoch_kl.append(float(kl.data.detach().cpu().numpy()))\n",
    "            epoch_loss.append(float(loss_.data.detach().cpu().numpy()))\n",
    "\n",
    "        hist_error.append(np.mean(epoch_error))\n",
    "        hist_loss.append(np.mean(epoch_loss))\n",
    "        hist_kl.append(np.mean(epoch_kl))\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "        print(\"Epoch %d -- loss %f, RMS error %f, KL %f\" % (epoch+1, hist_loss[-1], hist_error[-1], hist_kl[-1]))\n",
    "\n",
    "\n",
    "    torch.save(scinet.state_dict(), SAVE_PATH)\n",
    "    print(f\"Model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.Tensor().to(device)\n",
    "for minibatch in dataloader:\n",
    "    time_series, fr, st, question, answer = (\n",
    "        minibatch['time_series'].to(device) / 5,\n",
    "        minibatch['fr'].to(device) / 5,\n",
    "        minibatch['st'].to(device) / 5,\n",
    "        minibatch['question'].to(device) / 5,\n",
    "        minibatch['answer'].to(device) / 5\n",
    "    )\n",
    "    inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "    outputs = answer\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    input, pred = scinet.forward(inputs)\n",
    "    data = torch.cat((input, pred), 1)\n",
    "    error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "    dataset = torch.cat((dataset, data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.detach().cpu().numpy())\n",
    "df.to_csv(\"scinet_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
