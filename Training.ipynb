{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models import SciNet\n",
    "import pandas as pd\n",
    "from utils import target_loss \n",
    "from loader import build_dataloader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = 80\n",
    "size = 150\n",
    "t_max = 5\n",
    "t = np.linspace(0, t_max, t_size)\n",
    "min_fr, max_fr = 0.01, 100\n",
    "fr = np.random.uniform(min_fr, max_fr, size)\n",
    "start_st, end_st = 0.01, 100\n",
    "st = np.logspace(np.log10(start_st), np.log10(end_st), size, endpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 84)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function that we generate the data with\n",
    "'''def f(t, st, fr):\n",
    "    return st**2 * fr * (1 - t/st - np.exp(-t/st))'''\n",
    "def f(t, st, fr):\n",
    "    return  st**2 + fr **2 + t\n",
    "data = []\n",
    "for st_ in st:\n",
    "    for fr_ in fr:\n",
    "        example = list(f(t, st_, fr_))\n",
    "        t_pred = np.random.uniform(0, t_max)\n",
    "        pred = f(t_pred,st_,fr_)\n",
    "        example.append(fr_)\n",
    "        example.append(st_)\n",
    "        example.append(t_pred)\n",
    "        example.append(pred)\n",
    "        data.append(example)\n",
    "data = np.array(data)\n",
    "colummns = [str(i) for i in range(t_size)]\n",
    "colummns.append(\"fr\")\n",
    "colummns.append(\"st\")\n",
    "colummns.append(\"t_pred\")\n",
    "colummns.append(\"pred\") \n",
    "df = pd.DataFrame(data,columns=colummns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup scinet model with 3 latent neurons\n",
    "scinet = SciNet(t_size,1,3,100)\n",
    "\n",
    "# Load and prepare training data\n",
    "dataloader = build_dataloader(batch_size =100, size=t_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: SGD lr 0.001000 -> 0.000992\n",
      "Epoch 1 -- loss 185.734283, RMS error 0.787579 \n",
      "Epoch 2: SGD lr 0.000992 -> 0.000985\n",
      "Epoch 2 -- loss 94.510986, RMS error 0.517777 \n",
      "Epoch 3: SGD lr 0.000985 -> 0.000977\n",
      "Epoch 3 -- loss 59.778454, RMS error 0.368741 \n",
      "Epoch 4: SGD lr 0.000977 -> 0.000970\n",
      "Epoch 4 -- loss 48.419079, RMS error 0.332723 \n",
      "Epoch 5: SGD lr 0.000970 -> 0.000962\n",
      "Epoch 5 -- loss 46.031620, RMS error 0.320696 \n",
      "Epoch 6: SGD lr 0.000962 -> 0.000954\n",
      "Epoch 6 -- loss 44.111805, RMS error 0.304555 \n",
      "Epoch 7: SGD lr 0.000954 -> 0.000947\n",
      "Epoch 7 -- loss 45.322041, RMS error 0.294003 \n",
      "Epoch 8: SGD lr 0.000947 -> 0.000939\n",
      "Epoch 8 -- loss 41.507080, RMS error 0.273516 \n",
      "Epoch 9: SGD lr 0.000939 -> 0.000931\n",
      "Epoch 9 -- loss 42.506229, RMS error 0.270787 \n",
      "Epoch 10: SGD lr 0.000931 -> 0.000924\n",
      "Epoch 10 -- loss 35.841282, RMS error 0.214470 \n",
      "Epoch 11: SGD lr 0.000924 -> 0.000916\n",
      "Epoch 11 -- loss 24.845491, RMS error 0.118114 \n",
      "Epoch 12: SGD lr 0.000916 -> 0.000909\n",
      "Epoch 12 -- loss 22.834450, RMS error 0.113485 \n",
      "Epoch 13: SGD lr 0.000909 -> 0.000901\n",
      "Epoch 13 -- loss 20.540583, RMS error 0.096814 \n",
      "Epoch 14: SGD lr 0.000901 -> 0.000893\n",
      "Epoch 14 -- loss 19.347776, RMS error 0.100016 \n",
      "Epoch 15: SGD lr 0.000893 -> 0.000886\n",
      "Epoch 15 -- loss 18.007013, RMS error 0.104229 \n",
      "Epoch 16: SGD lr 0.000886 -> 0.000878\n",
      "Epoch 16 -- loss 15.540052, RMS error 0.106424 \n",
      "Epoch 17: SGD lr 0.000878 -> 0.000870\n",
      "Epoch 17 -- loss 12.955049, RMS error 0.100352 \n",
      "Epoch 18: SGD lr 0.000870 -> 0.000863\n",
      "Epoch 18 -- loss 11.296147, RMS error 0.092594 \n",
      "Epoch 19: SGD lr 0.000863 -> 0.000855\n",
      "Epoch 19 -- loss 10.602001, RMS error 0.089908 \n",
      "Epoch 20: SGD lr 0.000855 -> 0.000848\n",
      "Epoch 20 -- loss 10.228308, RMS error 0.087283 \n",
      "Epoch 21: SGD lr 0.000848 -> 0.000840\n",
      "Epoch 21 -- loss 10.043441, RMS error 0.085500 \n",
      "Epoch 22: SGD lr 0.000840 -> 0.000832\n",
      "Epoch 22 -- loss 10.134991, RMS error 0.088236 \n",
      "Epoch 23: SGD lr 0.000832 -> 0.000825\n",
      "Epoch 23 -- loss 10.372026, RMS error 0.092687 \n",
      "Epoch 24: SGD lr 0.000825 -> 0.000817\n",
      "Epoch 24 -- loss 9.635699, RMS error 0.079515 \n",
      "Epoch 25: SGD lr 0.000817 -> 0.000809\n",
      "Epoch 25 -- loss 9.191956, RMS error 0.071933 \n",
      "Epoch 26: SGD lr 0.000809 -> 0.000802\n",
      "Epoch 26 -- loss 9.223956, RMS error 0.074555 \n",
      "Epoch 27: SGD lr 0.000802 -> 0.000794\n",
      "Epoch 27 -- loss 9.285798, RMS error 0.075330 \n",
      "Epoch 28: SGD lr 0.000794 -> 0.000787\n",
      "Epoch 28 -- loss 9.498856, RMS error 0.081009 \n",
      "Epoch 29: SGD lr 0.000787 -> 0.000779\n",
      "Epoch 29 -- loss 9.101098, RMS error 0.074578 \n",
      "Epoch 30: SGD lr 0.000779 -> 0.000771\n",
      "Epoch 30 -- loss 9.259651, RMS error 0.076917 \n",
      "Epoch 31: SGD lr 0.000771 -> 0.000764\n",
      "Epoch 31 -- loss 9.033801, RMS error 0.074126 \n",
      "Epoch 32: SGD lr 0.000764 -> 0.000756\n",
      "Epoch 32 -- loss 8.929314, RMS error 0.070873 \n",
      "Epoch 33: SGD lr 0.000756 -> 0.000748\n",
      "Epoch 33 -- loss 9.506509, RMS error 0.083366 \n",
      "Epoch 34: SGD lr 0.000748 -> 0.000741\n",
      "Epoch 34 -- loss 8.908926, RMS error 0.071215 \n",
      "Epoch 35: SGD lr 0.000741 -> 0.000733\n",
      "Epoch 35 -- loss 8.826969, RMS error 0.070389 \n",
      "Epoch 36: SGD lr 0.000733 -> 0.000726\n",
      "Epoch 36 -- loss 9.011198, RMS error 0.075033 \n",
      "Epoch 37: SGD lr 0.000726 -> 0.000718\n",
      "Epoch 37 -- loss 8.772445, RMS error 0.068361 \n",
      "Epoch 38: SGD lr 0.000718 -> 0.000710\n",
      "Epoch 38 -- loss 9.382051, RMS error 0.079666 \n",
      "Epoch 39: SGD lr 0.000710 -> 0.000703\n",
      "Epoch 39 -- loss 9.091105, RMS error 0.075062 \n",
      "Epoch 40: SGD lr 0.000703 -> 0.000695\n",
      "Epoch 40 -- loss 8.864268, RMS error 0.070178 \n",
      "Epoch 41: SGD lr 0.000695 -> 0.000687\n",
      "Epoch 41 -- loss 8.766458, RMS error 0.069330 \n",
      "Epoch 42: SGD lr 0.000687 -> 0.000680\n",
      "Epoch 42 -- loss 8.694656, RMS error 0.068861 \n",
      "Epoch 43: SGD lr 0.000680 -> 0.000672\n",
      "Epoch 43 -- loss 8.631416, RMS error 0.067356 \n",
      "Epoch 44: SGD lr 0.000672 -> 0.000665\n",
      "Epoch 44 -- loss 8.671106, RMS error 0.067685 \n",
      "Epoch 45: SGD lr 0.000665 -> 0.000657\n",
      "Epoch 45 -- loss 8.628114, RMS error 0.065771 \n",
      "Epoch 46: SGD lr 0.000657 -> 0.000649\n",
      "Epoch 46 -- loss 8.471712, RMS error 0.062428 \n",
      "Epoch 47: SGD lr 0.000649 -> 0.000642\n",
      "Epoch 47 -- loss 8.541956, RMS error 0.065634 \n",
      "Epoch 48: SGD lr 0.000642 -> 0.000634\n",
      "Epoch 48 -- loss 8.926565, RMS error 0.073196 \n",
      "Epoch 49: SGD lr 0.000634 -> 0.000626\n",
      "Epoch 49 -- loss 9.126449, RMS error 0.076151 \n",
      "Epoch 50: SGD lr 0.000626 -> 0.000619\n",
      "Epoch 50 -- loss 8.559920, RMS error 0.065972 \n",
      "Epoch 51: SGD lr 0.000619 -> 0.000611\n",
      "Epoch 51 -- loss 8.650339, RMS error 0.068155 \n",
      "Epoch 52: SGD lr 0.000611 -> 0.000604\n",
      "Epoch 52 -- loss 8.345624, RMS error 0.061460 \n",
      "Epoch 53: SGD lr 0.000604 -> 0.000596\n",
      "Epoch 53 -- loss 8.720460, RMS error 0.069629 \n",
      "Epoch 54: SGD lr 0.000596 -> 0.000588\n",
      "Epoch 54 -- loss 8.793979, RMS error 0.069254 \n",
      "Epoch 55: SGD lr 0.000588 -> 0.000581\n",
      "Epoch 55 -- loss 8.637267, RMS error 0.066027 \n",
      "Epoch 56: SGD lr 0.000581 -> 0.000573\n",
      "Epoch 56 -- loss 8.531601, RMS error 0.064267 \n",
      "Epoch 57: SGD lr 0.000573 -> 0.000565\n",
      "Epoch 57 -- loss 8.318369, RMS error 0.059930 \n",
      "Epoch 58: SGD lr 0.000565 -> 0.000558\n",
      "Epoch 58 -- loss 8.607984, RMS error 0.067241 \n",
      "Epoch 59: SGD lr 0.000558 -> 0.000550\n",
      "Epoch 59 -- loss 8.387495, RMS error 0.060609 \n",
      "Epoch 60: SGD lr 0.000550 -> 0.000543\n",
      "Epoch 60 -- loss 8.370116, RMS error 0.060646 \n",
      "Epoch 61: SGD lr 0.000543 -> 0.000535\n",
      "Epoch 61 -- loss 8.319418, RMS error 0.060487 \n",
      "Epoch 62: SGD lr 0.000535 -> 0.000527\n",
      "Epoch 62 -- loss 8.257932, RMS error 0.060720 \n",
      "Epoch 63: SGD lr 0.000527 -> 0.000520\n",
      "Epoch 63 -- loss 8.403423, RMS error 0.062377 \n",
      "Epoch 64: SGD lr 0.000520 -> 0.000512\n",
      "Epoch 64 -- loss 8.517262, RMS error 0.066205 \n",
      "Epoch 65: SGD lr 0.000512 -> 0.000504\n",
      "Epoch 65 -- loss 8.504208, RMS error 0.064296 \n",
      "Epoch 66: SGD lr 0.000504 -> 0.000497\n",
      "Epoch 66 -- loss 8.260994, RMS error 0.060906 \n",
      "Epoch 67: SGD lr 0.000497 -> 0.000489\n",
      "Epoch 67 -- loss 8.451164, RMS error 0.063170 \n",
      "Epoch 68: SGD lr 0.000489 -> 0.000482\n",
      "Epoch 68 -- loss 8.299057, RMS error 0.062571 \n",
      "Epoch 69: SGD lr 0.000482 -> 0.000474\n",
      "Epoch 69 -- loss 8.350412, RMS error 0.061412 \n",
      "Epoch 70: SGD lr 0.000474 -> 0.000466\n",
      "Epoch 70 -- loss 8.653701, RMS error 0.069281 \n",
      "Epoch 71: SGD lr 0.000466 -> 0.000459\n",
      "Epoch 71 -- loss 8.247080, RMS error 0.059986 \n",
      "Epoch 72: SGD lr 0.000459 -> 0.000451\n",
      "Epoch 72 -- loss 8.158509, RMS error 0.058943 \n",
      "Epoch 73: SGD lr 0.000451 -> 0.000444\n",
      "Epoch 73 -- loss 8.275506, RMS error 0.061073 \n",
      "Epoch 74: SGD lr 0.000444 -> 0.000436\n",
      "Epoch 74 -- loss 8.408509, RMS error 0.063258 \n",
      "Epoch 75: SGD lr 0.000436 -> 0.000428\n",
      "Epoch 75 -- loss 8.331432, RMS error 0.060506 \n",
      "Epoch 76: SGD lr 0.000428 -> 0.000421\n",
      "Epoch 76 -- loss 8.117141, RMS error 0.056609 \n",
      "Epoch 77: SGD lr 0.000421 -> 0.000413\n",
      "Epoch 77 -- loss 8.338274, RMS error 0.056505 \n",
      "Epoch 78: SGD lr 0.000413 -> 0.000405\n",
      "Epoch 78 -- loss 8.128708, RMS error 0.055926 \n",
      "Epoch 79: SGD lr 0.000405 -> 0.000398\n",
      "Epoch 79 -- loss 8.189116, RMS error 0.058732 \n",
      "Epoch 80: SGD lr 0.000398 -> 0.000390\n",
      "Epoch 80 -- loss 8.088118, RMS error 0.055337 \n",
      "Epoch 81: SGD lr 0.000390 -> 0.000383\n",
      "Epoch 81 -- loss 8.097640, RMS error 0.056640 \n",
      "Epoch 82: SGD lr 0.000383 -> 0.000375\n",
      "Epoch 82 -- loss 8.023002, RMS error 0.056342 \n",
      "Epoch 83: SGD lr 0.000375 -> 0.000367\n",
      "Epoch 83 -- loss 8.085960, RMS error 0.059092 \n",
      "Epoch 84: SGD lr 0.000367 -> 0.000360\n",
      "Epoch 84 -- loss 8.150584, RMS error 0.057160 \n",
      "Epoch 85: SGD lr 0.000360 -> 0.000352\n",
      "Epoch 85 -- loss 8.096188, RMS error 0.056325 \n",
      "Epoch 86: SGD lr 0.000352 -> 0.000344\n",
      "Epoch 86 -- loss 8.118992, RMS error 0.054934 \n",
      "Epoch 87: SGD lr 0.000344 -> 0.000337\n",
      "Epoch 87 -- loss 8.086143, RMS error 0.056250 \n",
      "Epoch 88: SGD lr 0.000337 -> 0.000329\n",
      "Epoch 88 -- loss 8.022680, RMS error 0.055317 \n",
      "Epoch 89: SGD lr 0.000329 -> 0.000322\n",
      "Epoch 89 -- loss 8.101353, RMS error 0.057213 \n",
      "Epoch 90: SGD lr 0.000322 -> 0.000314\n",
      "Epoch 90 -- loss 7.950585, RMS error 0.053896 \n",
      "Epoch 91: SGD lr 0.000314 -> 0.000306\n",
      "Epoch 91 -- loss 8.081880, RMS error 0.054686 \n",
      "Epoch 92: SGD lr 0.000306 -> 0.000299\n",
      "Epoch 92 -- loss 8.132072, RMS error 0.055538 \n",
      "Epoch 93: SGD lr 0.000299 -> 0.000291\n",
      "Epoch 93 -- loss 8.146429, RMS error 0.059103 \n",
      "Epoch 94: SGD lr 0.000291 -> 0.000283\n",
      "Epoch 94 -- loss 8.042057, RMS error 0.056058 \n",
      "Epoch 95: SGD lr 0.000283 -> 0.000276\n",
      "Epoch 95 -- loss 8.379622, RMS error 0.061593 \n",
      "Epoch 96: SGD lr 0.000276 -> 0.000268\n",
      "Epoch 96 -- loss 8.075293, RMS error 0.055086 \n",
      "Epoch 97: SGD lr 0.000268 -> 0.000261\n",
      "Epoch 97 -- loss 8.003798, RMS error 0.053511 \n",
      "Epoch 98: SGD lr 0.000261 -> 0.000253\n",
      "Epoch 98 -- loss 8.015148, RMS error 0.054324 \n",
      "Epoch 99: SGD lr 0.000253 -> 0.000245\n",
      "Epoch 99 -- loss 7.968693, RMS error 0.052286 \n",
      "Epoch 100: SGD lr 0.000245 -> 0.000238\n",
      "Epoch 100 -- loss 7.939091, RMS error 0.051647 \n",
      "Epoch 101: SGD lr 0.000238 -> 0.000230\n",
      "Epoch 101 -- loss 7.962537, RMS error 0.052256 \n",
      "Epoch 102: SGD lr 0.000230 -> 0.000222\n",
      "Epoch 102 -- loss 7.935357, RMS error 0.052586 \n",
      "Epoch 103: SGD lr 0.000222 -> 0.000215\n",
      "Epoch 103 -- loss 8.083263, RMS error 0.054437 \n",
      "Epoch 104: SGD lr 0.000215 -> 0.000207\n",
      "Epoch 104 -- loss 7.952718, RMS error 0.051886 \n",
      "Epoch 105: SGD lr 0.000207 -> 0.000200\n",
      "Epoch 105 -- loss 7.959565, RMS error 0.052902 \n",
      "Epoch 106: SGD lr 0.000200 -> 0.000192\n",
      "Epoch 106 -- loss 7.925231, RMS error 0.051565 \n",
      "Epoch 107: SGD lr 0.000192 -> 0.000184\n",
      "Epoch 107 -- loss 7.953891, RMS error 0.051098 \n",
      "Epoch 108: SGD lr 0.000184 -> 0.000177\n",
      "Epoch 108 -- loss 7.916716, RMS error 0.050144 \n",
      "Epoch 109: SGD lr 0.000177 -> 0.000169\n",
      "Epoch 109 -- loss 7.919116, RMS error 0.051674 \n",
      "Epoch 110: SGD lr 0.000169 -> 0.000161\n",
      "Epoch 110 -- loss 7.851619, RMS error 0.050478 \n",
      "Epoch 111: SGD lr 0.000161 -> 0.000154\n",
      "Epoch 111 -- loss 7.864943, RMS error 0.051049 \n",
      "Epoch 112: SGD lr 0.000154 -> 0.000146\n",
      "Epoch 112 -- loss 7.836893, RMS error 0.051354 \n",
      "Epoch 113: SGD lr 0.000146 -> 0.000139\n",
      "Epoch 113 -- loss 7.878945, RMS error 0.050864 \n",
      "Epoch 114: SGD lr 0.000139 -> 0.000131\n",
      "Epoch 114 -- loss 7.866013, RMS error 0.050723 \n",
      "Epoch 115: SGD lr 0.000131 -> 0.000123\n",
      "Epoch 115 -- loss 7.889035, RMS error 0.050958 \n",
      "Epoch 116: SGD lr 0.000123 -> 0.000116\n",
      "Epoch 116 -- loss 7.802874, RMS error 0.049608 \n",
      "Epoch 117: SGD lr 0.000116 -> 0.000108\n",
      "Epoch 117 -- loss 7.840689, RMS error 0.050725 \n",
      "Epoch 118: SGD lr 0.000108 -> 0.000100\n",
      "Epoch 118 -- loss 7.856112, RMS error 0.050989 \n",
      "Epoch 119: SGD lr 0.000100 -> 0.000093\n",
      "Epoch 119 -- loss 7.815064, RMS error 0.049103 \n",
      "Epoch 120: SGD lr 0.000093 -> 0.000085\n",
      "Epoch 120 -- loss 7.811312, RMS error 0.049223 \n",
      "Epoch 121: SGD lr 0.000085 -> 0.000078\n",
      "Epoch 121 -- loss 7.848925, RMS error 0.049647 \n",
      "Epoch 122: SGD lr 0.000078 -> 0.000070\n",
      "Epoch 122 -- loss 7.867027, RMS error 0.049259 \n",
      "Epoch 123: SGD lr 0.000070 -> 0.000062\n",
      "Epoch 123 -- loss 7.801028, RMS error 0.047447 \n",
      "Epoch 124: SGD lr 0.000062 -> 0.000055\n",
      "Epoch 124 -- loss 7.828586, RMS error 0.048264 \n",
      "Epoch 125: SGD lr 0.000055 -> 0.000047\n",
      "Epoch 125 -- loss 7.779841, RMS error 0.047062 \n",
      "Epoch 126: SGD lr 0.000047 -> 0.000039\n",
      "Epoch 126 -- loss 7.775731, RMS error 0.047577 \n",
      "Epoch 127: SGD lr 0.000039 -> 0.000032\n",
      "Epoch 127 -- loss 7.795698, RMS error 0.048264 \n",
      "Epoch 128: SGD lr 0.000032 -> 0.000024\n",
      "Epoch 128 -- loss 7.785203, RMS error 0.047540 \n",
      "Epoch 129: SGD lr 0.000024 -> 0.000017\n",
      "Epoch 129 -- loss 7.817437, RMS error 0.047406 \n",
      "Epoch 130: SGD lr 0.000017 -> 0.000009\n",
      "Epoch 130 -- loss 7.799314, RMS error 0.047346 \n",
      "Model saved to trained_models/scinet1.dat\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "SAVE_PATH = \"trained_models/scinet1.dat\"\n",
    "N_EPOCHS = 130\n",
    "optimizer = optim.Adam(scinet.parameters(), lr=0.001)\n",
    "hist_error = []\n",
    "hist_loss = []\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.009, total_iters=N_EPOCHS)\n",
    "beta = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    epoch_error = []\n",
    "    epoch_loss = []\n",
    "    for i_batch, minibatch in enumerate(dataloader):\n",
    "        time_series, _, _, question, answer = minibatch['time_series'] / 5, minibatch['fr'] / 5, minibatch['st'] / 5, minibatch['question'] / 5, minibatch['answer'] / 5\n",
    "        \n",
    "        # concat the time series with the features\n",
    "        inputs = torch.cat((time_series, question.reshape(-1, 1)), 1)\n",
    "        # build the output\n",
    "        outputs = answer\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(inputs[0])\n",
    "        pred = scinet.forward(inputs)\n",
    "        # print(pred)\n",
    "        # break\n",
    "        loss = target_loss(pred, outputs) + beta * scinet.kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        error = torch.mean(torch.sqrt((pred[:,0]-outputs)**2)).detach().numpy()\n",
    "        epoch_error.append(error)\n",
    "        epoch_loss.append(loss.data.detach().numpy())\n",
    "    # break\n",
    "    hist_error.append(np.mean(epoch_error))\n",
    "    hist_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "    \n",
    "    print(\"Epoch %d -- loss %f, RMS error %f \" % (epoch+1, hist_loss[-1], hist_error[-1]))\n",
    "torch.save(scinet.state_dict(), SAVE_PATH)\n",
    "print(\"Model saved to %s\" % SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Window Means:\", window_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot some training history data\n",
    "%matplotlib inline \n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(hist_error)\n",
    "ax1.set_ylabel(\"Amplitude RMSE\")\n",
    "ax2.plot(hist_loss)\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
