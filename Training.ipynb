{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models import SciNet\n",
    "import pandas as pd\n",
    "from utils import target_loss \n",
    "from loader import build_dataloader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = 80\n",
    "size = 150\n",
    "t_max = 5\n",
    "t = np.linspace(0, t_max, t_size)\n",
    "min_fr, max_fr = 0, 5\n",
    "fr = np.random.uniform(min_fr, max_fr, size)\n",
    "start_st, end_st = 0.01, 100\n",
    "st = np.logspace(np.log10(start_st), np.log10(end_st), size, endpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m colummns\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mt_pred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m colummns\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m---> 21\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data,columns\u001b[39m=\u001b[39mcolummns)\n\u001b[0;32m     22\u001b[0m df\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'DataFrame'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# the function that we generate the data with\n",
    "def f(t, st, fr):\n",
    "    return st**2 * fr * (1 - t/st - np.exp(-t/st))\n",
    "data = []\n",
    "for st_ in st:\n",
    "    for fr_ in fr:\n",
    "        example = list(f(t, st_, fr_))\n",
    "        t_pred = np.random.uniform(0, t_max)\n",
    "        pred = f(t_pred,st_,fr_)\n",
    "        example.append(fr_)\n",
    "        example.append(st_)\n",
    "        example.append(t_pred)\n",
    "        example.append(pred)\n",
    "        data.append(example)\n",
    "data = np.array(data)\n",
    "colummns = [str(i) for i in range(t_size)]\n",
    "colummns.append(\"fr\")\n",
    "colummns.append(\"st\")\n",
    "colummns.append(\"t_pred\")\n",
    "colummns.append(\"pred\") \n",
    "df = pd.DataFrame(data,columns=colummns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup scinet model with 3 latent neurons\n",
    "scinet = SciNet(100,1,3,64)\n",
    "\n",
    "# Load and prepare training data\n",
    "dataloader = build_dataloader(batch_size = 64, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: SGD lr 0.001000 -> 0.000993\n",
      "Epoch 1 -- loss 2481.484375, RMS error 3.346748 \n",
      "Epoch 1: SGD lr 0.000993 -> 0.000987\n",
      "Epoch 2 -- loss 838.726624, RMS error 1.905076 \n",
      "Epoch 2: SGD lr 0.000987 -> 0.000980\n",
      "Epoch 3 -- loss 380.560669, RMS error 1.219364 \n",
      "Epoch 3: SGD lr 0.000980 -> 0.000974\n",
      "Epoch 4 -- loss 223.629456, RMS error 0.869998 \n",
      "Epoch 4: SGD lr 0.000974 -> 0.000967\n",
      "Epoch 5 -- loss 158.648224, RMS error 0.693198 \n",
      "Epoch 5: SGD lr 0.000967 -> 0.000960\n",
      "Epoch 6 -- loss 114.344681, RMS error 0.579856 \n",
      "Epoch 6: SGD lr 0.000960 -> 0.000954\n",
      "Epoch 7 -- loss 93.115265, RMS error 0.510542 \n",
      "Epoch 7: SGD lr 0.000954 -> 0.000947\n",
      "Epoch 8 -- loss 74.616615, RMS error 0.440538 \n",
      "Epoch 8: SGD lr 0.000947 -> 0.000941\n",
      "Epoch 9 -- loss 66.042557, RMS error 0.430318 \n",
      "Epoch 9: SGD lr 0.000941 -> 0.000934\n",
      "Epoch 10 -- loss 57.684010, RMS error 0.386093 \n",
      "Epoch 10: SGD lr 0.000934 -> 0.000927\n",
      "Epoch 11 -- loss 53.118954, RMS error 0.376733 \n",
      "Epoch 11: SGD lr 0.000927 -> 0.000921\n",
      "Epoch 12 -- loss 45.684708, RMS error 0.336127 \n",
      "Epoch 12: SGD lr 0.000921 -> 0.000914\n",
      "Epoch 13 -- loss 45.116543, RMS error 0.344267 \n",
      "Epoch 13: SGD lr 0.000914 -> 0.000908\n",
      "Epoch 14 -- loss 40.466412, RMS error 0.319082 \n",
      "Epoch 14: SGD lr 0.000908 -> 0.000901\n",
      "Epoch 15 -- loss 46.246338, RMS error 0.364709 \n",
      "Epoch 15: SGD lr 0.000901 -> 0.000894\n",
      "Epoch 16 -- loss 35.914616, RMS error 0.288978 \n",
      "Epoch 16: SGD lr 0.000894 -> 0.000888\n",
      "Epoch 17 -- loss 35.577129, RMS error 0.294998 \n",
      "Epoch 17: SGD lr 0.000888 -> 0.000881\n",
      "Epoch 18 -- loss 33.706745, RMS error 0.283363 \n",
      "Epoch 18: SGD lr 0.000881 -> 0.000874\n",
      "Epoch 19 -- loss 31.000050, RMS error 0.254717 \n",
      "Epoch 19: SGD lr 0.000874 -> 0.000868\n",
      "Epoch 20 -- loss 30.444954, RMS error 0.252174 \n",
      "Epoch 20: SGD lr 0.000868 -> 0.000861\n",
      "Epoch 21 -- loss 29.846220, RMS error 0.256628 \n",
      "Epoch 21: SGD lr 0.000861 -> 0.000855\n",
      "Epoch 22 -- loss 28.836525, RMS error 0.250568 \n",
      "Epoch 22: SGD lr 0.000855 -> 0.000848\n",
      "Epoch 23 -- loss 27.543373, RMS error 0.240976 \n",
      "Epoch 23: SGD lr 0.000848 -> 0.000841\n",
      "Epoch 24 -- loss 26.673691, RMS error 0.241974 \n",
      "Epoch 24: SGD lr 0.000841 -> 0.000835\n",
      "Epoch 25 -- loss 27.259922, RMS error 0.239921 \n",
      "Epoch 25: SGD lr 0.000835 -> 0.000828\n",
      "Epoch 26 -- loss 25.551136, RMS error 0.231218 \n",
      "Epoch 26: SGD lr 0.000828 -> 0.000822\n",
      "Epoch 27 -- loss 24.602253, RMS error 0.223880 \n",
      "Epoch 27: SGD lr 0.000822 -> 0.000815\n",
      "Epoch 28 -- loss 23.618675, RMS error 0.215273 \n",
      "Epoch 28: SGD lr 0.000815 -> 0.000808\n",
      "Epoch 29 -- loss 24.084024, RMS error 0.224952 \n",
      "Epoch 29: SGD lr 0.000808 -> 0.000802\n",
      "Epoch 30 -- loss 22.929850, RMS error 0.220232 \n",
      "Epoch 30: SGD lr 0.000802 -> 0.000795\n",
      "Epoch 31 -- loss 21.287762, RMS error 0.198261 \n",
      "Epoch 31: SGD lr 0.000795 -> 0.000789\n",
      "Epoch 32 -- loss 21.795729, RMS error 0.199647 \n",
      "Epoch 32: SGD lr 0.000789 -> 0.000782\n",
      "Epoch 33 -- loss 21.125273, RMS error 0.194458 \n",
      "Epoch 33: SGD lr 0.000782 -> 0.000775\n",
      "Epoch 34 -- loss 21.021154, RMS error 0.201447 \n",
      "Epoch 34: SGD lr 0.000775 -> 0.000769\n",
      "Epoch 35 -- loss 19.428509, RMS error 0.180097 \n",
      "Epoch 35: SGD lr 0.000769 -> 0.000762\n",
      "Epoch 36 -- loss 24.171673, RMS error 0.234022 \n",
      "Epoch 36: SGD lr 0.000762 -> 0.000756\n",
      "Epoch 37 -- loss 19.521412, RMS error 0.185541 \n",
      "Epoch 37: SGD lr 0.000756 -> 0.000749\n",
      "Epoch 38 -- loss 20.486126, RMS error 0.206364 \n",
      "Epoch 38: SGD lr 0.000749 -> 0.000742\n",
      "Epoch 39 -- loss 20.316484, RMS error 0.197637 \n",
      "Epoch 39: SGD lr 0.000742 -> 0.000736\n",
      "Epoch 40 -- loss 19.960562, RMS error 0.193590 \n",
      "Epoch 40: SGD lr 0.000736 -> 0.000729\n",
      "Epoch 41 -- loss 20.812422, RMS error 0.209997 \n",
      "Epoch 41: SGD lr 0.000729 -> 0.000723\n",
      "Epoch 42 -- loss 18.299711, RMS error 0.168266 \n",
      "Epoch 42: SGD lr 0.000723 -> 0.000716\n",
      "Epoch 43 -- loss 17.630468, RMS error 0.164343 \n",
      "Epoch 43: SGD lr 0.000716 -> 0.000709\n",
      "Epoch 44 -- loss 21.655624, RMS error 0.220050 \n",
      "Epoch 44: SGD lr 0.000709 -> 0.000703\n",
      "Epoch 45 -- loss 19.661158, RMS error 0.200662 \n",
      "Epoch 45: SGD lr 0.000703 -> 0.000696\n",
      "Epoch 46 -- loss 17.777529, RMS error 0.171393 \n",
      "Epoch 46: SGD lr 0.000696 -> 0.000689\n",
      "Epoch 47 -- loss 18.409060, RMS error 0.183353 \n",
      "Epoch 47: SGD lr 0.000689 -> 0.000683\n",
      "Epoch 48 -- loss 17.109724, RMS error 0.160538 \n",
      "Epoch 48: SGD lr 0.000683 -> 0.000676\n",
      "Epoch 49 -- loss 17.100212, RMS error 0.165043 \n",
      "Epoch 49: SGD lr 0.000676 -> 0.000670\n",
      "Epoch 50 -- loss 16.409821, RMS error 0.153463 \n",
      "Epoch 50: SGD lr 0.000670 -> 0.000663\n",
      "Epoch 51 -- loss 16.731962, RMS error 0.162872 \n",
      "Epoch 51: SGD lr 0.000663 -> 0.000656\n",
      "Epoch 52 -- loss 17.607498, RMS error 0.178767 \n",
      "Epoch 52: SGD lr 0.000656 -> 0.000650\n",
      "Epoch 53 -- loss 16.753284, RMS error 0.165720 \n",
      "Epoch 53: SGD lr 0.000650 -> 0.000643\n",
      "Epoch 54 -- loss 16.311375, RMS error 0.156941 \n",
      "Epoch 54: SGD lr 0.000643 -> 0.000637\n",
      "Epoch 55 -- loss 15.989264, RMS error 0.155130 \n",
      "Epoch 55: SGD lr 0.000637 -> 0.000630\n",
      "Epoch 56 -- loss 18.297998, RMS error 0.197921 \n",
      "Epoch 56: SGD lr 0.000630 -> 0.000623\n",
      "Epoch 57 -- loss 16.207413, RMS error 0.155900 \n",
      "Epoch 57: SGD lr 0.000623 -> 0.000617\n",
      "Epoch 58 -- loss 16.461843, RMS error 0.164514 \n",
      "Epoch 58: SGD lr 0.000617 -> 0.000610\n",
      "Epoch 59 -- loss 15.673788, RMS error 0.147038 \n",
      "Epoch 59: SGD lr 0.000610 -> 0.000604\n",
      "Epoch 60 -- loss 15.216005, RMS error 0.140962 \n",
      "Epoch 60: SGD lr 0.000604 -> 0.000597\n",
      "Epoch 61 -- loss 16.339073, RMS error 0.163050 \n",
      "Epoch 61: SGD lr 0.000597 -> 0.000590\n",
      "Epoch 62 -- loss 15.372287, RMS error 0.146500 \n",
      "Epoch 62: SGD lr 0.000590 -> 0.000584\n",
      "Epoch 63 -- loss 15.468128, RMS error 0.146021 \n",
      "Epoch 63: SGD lr 0.000584 -> 0.000577\n",
      "Epoch 64 -- loss 16.144339, RMS error 0.168861 \n",
      "Epoch 64: SGD lr 0.000577 -> 0.000571\n",
      "Epoch 65 -- loss 15.450155, RMS error 0.152937 \n",
      "Epoch 65: SGD lr 0.000571 -> 0.000564\n",
      "Epoch 66 -- loss 15.684732, RMS error 0.158268 \n",
      "Epoch 66: SGD lr 0.000564 -> 0.000557\n",
      "Epoch 67 -- loss 15.344577, RMS error 0.147695 \n",
      "Epoch 67: SGD lr 0.000557 -> 0.000551\n",
      "Epoch 68 -- loss 15.421835, RMS error 0.151979 \n",
      "Epoch 68: SGD lr 0.000551 -> 0.000544\n",
      "Epoch 69 -- loss 16.163439, RMS error 0.165722 \n",
      "Epoch 69: SGD lr 0.000544 -> 0.000538\n",
      "Epoch 70 -- loss 15.573000, RMS error 0.154786 \n",
      "Epoch 70: SGD lr 0.000538 -> 0.000531\n",
      "Epoch 71 -- loss 14.988607, RMS error 0.144764 \n",
      "Epoch 71: SGD lr 0.000531 -> 0.000524\n",
      "Epoch 72 -- loss 14.931006, RMS error 0.145053 \n",
      "Epoch 72: SGD lr 0.000524 -> 0.000518\n",
      "Epoch 73 -- loss 14.883121, RMS error 0.143982 \n",
      "Epoch 73: SGD lr 0.000518 -> 0.000511\n",
      "Epoch 74 -- loss 14.552690, RMS error 0.138001 \n",
      "Epoch 74: SGD lr 0.000511 -> 0.000504\n",
      "Epoch 75 -- loss 14.982123, RMS error 0.153983 \n",
      "Epoch 75: SGD lr 0.000504 -> 0.000498\n",
      "Epoch 76 -- loss 18.567505, RMS error 0.206261 \n",
      "Epoch 76: SGD lr 0.000498 -> 0.000491\n",
      "Epoch 77 -- loss 15.034725, RMS error 0.147380 \n",
      "Epoch 77: SGD lr 0.000491 -> 0.000485\n",
      "Epoch 78 -- loss 14.688925, RMS error 0.136430 \n",
      "Epoch 78: SGD lr 0.000485 -> 0.000478\n",
      "Epoch 79 -- loss 14.634031, RMS error 0.139776 \n",
      "Epoch 79: SGD lr 0.000478 -> 0.000471\n",
      "Epoch 80 -- loss 14.256643, RMS error 0.133120 \n",
      "Epoch 80: SGD lr 0.000471 -> 0.000465\n",
      "Epoch 81 -- loss 14.031620, RMS error 0.129304 \n",
      "Epoch 81: SGD lr 0.000465 -> 0.000458\n",
      "Epoch 82 -- loss 14.198565, RMS error 0.133279 \n",
      "Epoch 82: SGD lr 0.000458 -> 0.000452\n",
      "Epoch 83 -- loss 14.237640, RMS error 0.133494 \n",
      "Epoch 83: SGD lr 0.000452 -> 0.000445\n",
      "Epoch 84 -- loss 14.591612, RMS error 0.143179 \n",
      "Epoch 84: SGD lr 0.000445 -> 0.000438\n",
      "Epoch 85 -- loss 14.695137, RMS error 0.143333 \n",
      "Epoch 85: SGD lr 0.000438 -> 0.000432\n",
      "Epoch 86 -- loss 13.790582, RMS error 0.120627 \n",
      "Epoch 86: SGD lr 0.000432 -> 0.000425\n",
      "Epoch 87 -- loss 14.459224, RMS error 0.141579 \n",
      "Epoch 87: SGD lr 0.000425 -> 0.000419\n",
      "Epoch 88 -- loss 14.862622, RMS error 0.150083 \n",
      "Epoch 88: SGD lr 0.000419 -> 0.000412\n",
      "Epoch 89 -- loss 13.946568, RMS error 0.127269 \n",
      "Epoch 89: SGD lr 0.000412 -> 0.000405\n",
      "Epoch 90 -- loss 14.518776, RMS error 0.143854 \n",
      "Epoch 90: SGD lr 0.000405 -> 0.000399\n",
      "Epoch 91 -- loss 14.651212, RMS error 0.146120 \n",
      "Epoch 91: SGD lr 0.000399 -> 0.000392\n",
      "Epoch 92 -- loss 13.824961, RMS error 0.126567 \n",
      "Epoch 92: SGD lr 0.000392 -> 0.000386\n",
      "Epoch 93 -- loss 14.009687, RMS error 0.130194 \n",
      "Epoch 93: SGD lr 0.000386 -> 0.000379\n",
      "Epoch 94 -- loss 13.780360, RMS error 0.125932 \n",
      "Epoch 94: SGD lr 0.000379 -> 0.000372\n",
      "Epoch 95 -- loss 14.027144, RMS error 0.130539 \n",
      "Epoch 95: SGD lr 0.000372 -> 0.000366\n",
      "Epoch 96 -- loss 13.863379, RMS error 0.128611 \n",
      "Epoch 96: SGD lr 0.000366 -> 0.000359\n",
      "Epoch 97 -- loss 14.592319, RMS error 0.149802 \n",
      "Epoch 97: SGD lr 0.000359 -> 0.000353\n",
      "Epoch 98 -- loss 13.664210, RMS error 0.122833 \n",
      "Epoch 98: SGD lr 0.000353 -> 0.000346\n",
      "Epoch 99 -- loss 13.928970, RMS error 0.132908 \n",
      "Epoch 99: SGD lr 0.000346 -> 0.000339\n",
      "Epoch 100 -- loss 14.376225, RMS error 0.144345 \n",
      "Epoch 100: SGD lr 0.000339 -> 0.000333\n",
      "Epoch 101 -- loss 13.583524, RMS error 0.121219 \n",
      "Epoch 101: SGD lr 0.000333 -> 0.000326\n",
      "Epoch 102 -- loss 15.277795, RMS error 0.156030 \n",
      "Epoch 102: SGD lr 0.000326 -> 0.000320\n",
      "Epoch 103 -- loss 13.368786, RMS error 0.116971 \n",
      "Epoch 103: SGD lr 0.000320 -> 0.000313\n",
      "Epoch 104 -- loss 13.541371, RMS error 0.118485 \n",
      "Epoch 104: SGD lr 0.000313 -> 0.000306\n",
      "Epoch 105 -- loss 13.537874, RMS error 0.121497 \n",
      "Epoch 105: SGD lr 0.000306 -> 0.000300\n",
      "Epoch 106 -- loss 13.254590, RMS error 0.114939 \n",
      "Epoch 106: SGD lr 0.000300 -> 0.000293\n",
      "Epoch 107 -- loss 13.609260, RMS error 0.124789 \n",
      "Epoch 107: SGD lr 0.000293 -> 0.000286\n",
      "Epoch 108 -- loss 13.246957, RMS error 0.112931 \n",
      "Epoch 108: SGD lr 0.000286 -> 0.000280\n",
      "Epoch 109 -- loss 13.283486, RMS error 0.115707 \n",
      "Epoch 109: SGD lr 0.000280 -> 0.000273\n",
      "Epoch 110 -- loss 13.733712, RMS error 0.128146 \n",
      "Epoch 110: SGD lr 0.000273 -> 0.000267\n",
      "Epoch 111 -- loss 13.316922, RMS error 0.116546 \n",
      "Epoch 111: SGD lr 0.000267 -> 0.000260\n",
      "Epoch 112 -- loss 13.468949, RMS error 0.123913 \n",
      "Epoch 112: SGD lr 0.000260 -> 0.000253\n",
      "Epoch 113 -- loss 13.227945, RMS error 0.117104 \n",
      "Epoch 113: SGD lr 0.000253 -> 0.000247\n",
      "Epoch 114 -- loss 13.086180, RMS error 0.112363 \n",
      "Epoch 114: SGD lr 0.000247 -> 0.000240\n",
      "Epoch 115 -- loss 13.279304, RMS error 0.119417 \n",
      "Epoch 115: SGD lr 0.000240 -> 0.000234\n",
      "Epoch 116 -- loss 13.351455, RMS error 0.120986 \n",
      "Epoch 116: SGD lr 0.000234 -> 0.000227\n",
      "Epoch 117 -- loss 13.203941, RMS error 0.114143 \n",
      "Epoch 117: SGD lr 0.000227 -> 0.000220\n",
      "Epoch 118 -- loss 13.235423, RMS error 0.116800 \n",
      "Epoch 118: SGD lr 0.000220 -> 0.000214\n",
      "Epoch 119 -- loss 12.957491, RMS error 0.109336 \n",
      "Epoch 119: SGD lr 0.000214 -> 0.000207\n",
      "Epoch 120 -- loss 13.044511, RMS error 0.112952 \n",
      "Epoch 120: SGD lr 0.000207 -> 0.000201\n",
      "Epoch 121 -- loss 12.993429, RMS error 0.110891 \n",
      "Epoch 121: SGD lr 0.000201 -> 0.000194\n",
      "Epoch 122 -- loss 13.066868, RMS error 0.114926 \n",
      "Epoch 122: SGD lr 0.000194 -> 0.000187\n",
      "Epoch 123 -- loss 13.067368, RMS error 0.114581 \n",
      "Epoch 123: SGD lr 0.000187 -> 0.000181\n",
      "Epoch 124 -- loss 13.152077, RMS error 0.116999 \n",
      "Epoch 124: SGD lr 0.000181 -> 0.000174\n",
      "Epoch 125 -- loss 13.154652, RMS error 0.118447 \n",
      "Epoch 125: SGD lr 0.000174 -> 0.000168\n",
      "Epoch 126 -- loss 13.172272, RMS error 0.118897 \n",
      "Epoch 126: SGD lr 0.000168 -> 0.000161\n",
      "Epoch 127 -- loss 13.265208, RMS error 0.120461 \n",
      "Epoch 127: SGD lr 0.000161 -> 0.000154\n",
      "Epoch 128 -- loss 12.948610, RMS error 0.111760 \n",
      "Epoch 128: SGD lr 0.000154 -> 0.000148\n",
      "Epoch 129 -- loss 12.985462, RMS error 0.110660 \n",
      "Epoch 129: SGD lr 0.000148 -> 0.000141\n",
      "Epoch 130 -- loss 12.905472, RMS error 0.110835 \n",
      "Epoch 130: SGD lr 0.000141 -> 0.000135\n",
      "Epoch 131 -- loss 12.822495, RMS error 0.109899 \n",
      "Epoch 131: SGD lr 0.000135 -> 0.000128\n",
      "Epoch 132 -- loss 12.811111, RMS error 0.108787 \n",
      "Epoch 132: SGD lr 0.000128 -> 0.000121\n",
      "Epoch 133 -- loss 12.900832, RMS error 0.110590 \n",
      "Epoch 133: SGD lr 0.000121 -> 0.000115\n",
      "Epoch 134 -- loss 12.798308, RMS error 0.106896 \n",
      "Epoch 134: SGD lr 0.000115 -> 0.000108\n",
      "Epoch 135 -- loss 12.843470, RMS error 0.106507 \n",
      "Epoch 135: SGD lr 0.000108 -> 0.000101\n",
      "Epoch 136 -- loss 12.826980, RMS error 0.106805 \n",
      "Epoch 136: SGD lr 0.000101 -> 0.000095\n",
      "Epoch 137 -- loss 12.786266, RMS error 0.105226 \n",
      "Epoch 137: SGD lr 0.000095 -> 0.000088\n",
      "Epoch 138 -- loss 12.770497, RMS error 0.105453 \n",
      "Epoch 138: SGD lr 0.000088 -> 0.000082\n",
      "Epoch 139 -- loss 12.715990, RMS error 0.104229 \n",
      "Epoch 139: SGD lr 0.000082 -> 0.000075\n",
      "Epoch 140 -- loss 12.681078, RMS error 0.104724 \n",
      "Epoch 140: SGD lr 0.000075 -> 0.000068\n",
      "Epoch 141 -- loss 12.692385, RMS error 0.104098 \n",
      "Epoch 141: SGD lr 0.000068 -> 0.000062\n",
      "Epoch 142 -- loss 12.688607, RMS error 0.103925 \n",
      "Epoch 142: SGD lr 0.000062 -> 0.000055\n",
      "Epoch 143 -- loss 12.630462, RMS error 0.103254 \n",
      "Epoch 143: SGD lr 0.000055 -> 0.000049\n",
      "Epoch 144 -- loss 12.813502, RMS error 0.106790 \n",
      "Epoch 144: SGD lr 0.000049 -> 0.000042\n",
      "Epoch 145 -- loss 12.651955, RMS error 0.101849 \n",
      "Epoch 145: SGD lr 0.000042 -> 0.000035\n",
      "Epoch 146 -- loss 12.664746, RMS error 0.102445 \n",
      "Epoch 146: SGD lr 0.000035 -> 0.000029\n",
      "Epoch 147 -- loss 12.581142, RMS error 0.100631 \n",
      "Epoch 147: SGD lr 0.000029 -> 0.000022\n",
      "Epoch 148 -- loss 12.624763, RMS error 0.101294 \n",
      "Epoch 148: SGD lr 0.000022 -> 0.000016\n",
      "Epoch 149 -- loss 12.598749, RMS error 0.099693 \n",
      "Epoch 149: SGD lr 0.000016 -> 0.000009\n",
      "Epoch 150 -- loss 12.648572, RMS error 0.100606 \n",
      "Model saved to trained_models/scinet1.dat\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "SAVE_PATH = \"trained_models/scinet1.dat\"\n",
    "N_EPOCHS = 150\n",
    "optimizer = optim.Adam(scinet.parameters(), lr=0.001)\n",
    "hist_error = []\n",
    "hist_loss = []\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.009, total_iters=N_EPOCHS)\n",
    "beta = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    epoch_error = []\n",
    "    epoch_loss = []\n",
    "    for i_batch, minibatch in enumerate(dataloader):\n",
    "        time_series, _, _, question, answer = minibatch['time_series'], minibatch['fr'], minibatch['st'], minibatch['question'], minibatch['answer']\n",
    "        \n",
    "        # concat the time series with the features\n",
    "        inputs = torch.cat((time_series, question.reshape(-1, 1)), 1)\n",
    "        # build the output\n",
    "        outputs = answer\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(inputs[0])\n",
    "        pred = scinet.forward(inputs)\n",
    "        # print(pred)\n",
    "        # break\n",
    "        loss = target_loss(pred, outputs) + beta * scinet.kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        error = torch.mean(torch.sqrt((pred[:,0]-outputs)**2)).detach().numpy()\n",
    "        epoch_error.append(error)\n",
    "        epoch_loss.append(loss.data.detach().numpy())\n",
    "    # break\n",
    "    hist_error.append(np.mean(epoch_error))\n",
    "    hist_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "    \n",
    "    print(\"Epoch %d -- loss %f, RMS error %f \" % (epoch+1, hist_loss[-1], hist_error[-1]))\n",
    "torch.save(scinet.state_dict(), SAVE_PATH)\n",
    "print(\"Model saved to %s\" % SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Window Means:\", window_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot some training history data\n",
    "%matplotlib inline \n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(hist_error)\n",
    "ax1.set_ylabel(\"Amplitude RMSE\")\n",
    "ax2.plot(hist_loss)\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
