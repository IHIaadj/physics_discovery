{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.         -4.8989899  -4.7979798  -4.6969697  -4.5959596  -4.49494949\n",
      " -4.39393939 -4.29292929 -4.19191919 -4.09090909 -3.98989899 -3.88888889\n",
      " -3.78787879 -3.68686869 -3.58585859 -3.48484848 -3.38383838 -3.28282828\n",
      " -3.18181818 -3.08080808 -2.97979798 -2.87878788 -2.77777778 -2.67676768\n",
      " -2.57575758 -2.47474747 -2.37373737 -2.27272727 -2.17171717 -2.07070707\n",
      " -1.96969697 -1.86868687 -1.76767677 -1.66666667 -1.56565657 -1.46464646\n",
      " -1.36363636 -1.26262626 -1.16161616 -1.06060606 -0.95959596 -0.85858586\n",
      " -0.75757576 -0.65656566 -0.55555556 -0.45454545 -0.35353535 -0.25252525\n",
      " -0.15151515 -0.05050505  0.05050505  0.15151515  0.25252525  0.35353535\n",
      "  0.45454545  0.55555556  0.65656566  0.75757576  0.85858586  0.95959596\n",
      "  1.06060606  1.16161616  1.26262626  1.36363636  1.46464646  1.56565657\n",
      "  1.66666667  1.76767677  1.86868687  1.96969697  2.07070707  2.17171717\n",
      "  2.27272727  2.37373737  2.47474747  2.57575758  2.67676768  2.77777778\n",
      "  2.87878788  2.97979798  3.08080808  3.18181818  3.28282828  3.38383838\n",
      "  3.48484848  3.58585859  3.68686869  3.78787879  3.88888889  3.98989899\n",
      "  4.09090909  4.19191919  4.29292929  4.39393939  4.49494949  4.5959596\n",
      "  4.6969697   4.7979798   4.8989899   5.        ]\n",
      "[-4.         -3.8989899  -3.7979798  -3.6969697  -3.5959596  -3.49494949\n",
      " -3.39393939 -3.29292929 -3.19191919 -3.09090909 -2.98989899 -2.88888889\n",
      " -2.78787879 -2.68686869 -2.58585859 -2.48484848 -2.38383838 -2.28282828\n",
      " -2.18181818 -2.08080808 -1.97979798 -1.87878788 -1.77777778 -1.67676768\n",
      " -1.57575758 -1.47474747 -1.37373737 -1.27272727 -1.17171717 -1.07070707\n",
      " -0.96969697 -0.86868687 -0.76767677 -0.66666667 -0.56565657 -0.46464646\n",
      " -0.36363636 -0.26262626 -0.16161616 -0.06060606  0.04040404  0.14141414\n",
      "  0.24242424  0.34343434  0.44444444  0.54545455  0.64646465  0.74747475\n",
      "  0.84848485  0.94949495  1.05050505  1.15151515  1.25252525  1.35353535\n",
      "  1.45454545  1.55555556  1.65656566  1.75757576  1.85858586  1.95959596\n",
      "  2.06060606  2.16161616  2.26262626  2.36363636  2.46464646  2.56565657\n",
      "  2.66666667  2.76767677  2.86868687  2.96969697  3.07070707  3.17171717\n",
      "  3.27272727  3.37373737  3.47474747  3.57575758  3.67676768  3.77777778\n",
      "  3.87878788  3.97979798  4.08080808  4.18181818  4.28282828  4.38383838\n",
      "  4.48484848  4.58585859  4.68686869  4.78787879  4.88888889  4.98989899\n",
      "  5.09090909  5.19191919  5.29292929  5.39393939  5.49494949  5.5959596\n",
      "  5.6969697   5.7979798   5.8989899   6.        ]\n",
      "Epoch [100/5000], Loss: 0.0003\n",
      "Epoch [200/5000], Loss: 0.0000\n",
      "Epoch [300/5000], Loss: 0.0000\n",
      "Epoch [400/5000], Loss: 0.0000\n",
      "Epoch [500/5000], Loss: 0.0000\n",
      "Epoch [600/5000], Loss: 0.0000\n",
      "Epoch [700/5000], Loss: 0.0000\n",
      "Epoch [800/5000], Loss: 0.0000\n",
      "Epoch [900/5000], Loss: 0.0000\n",
      "Epoch [1000/5000], Loss: 0.0000\n",
      "Epoch [1100/5000], Loss: 0.0000\n",
      "Epoch [1200/5000], Loss: 0.0000\n",
      "Epoch [1300/5000], Loss: 0.0000\n",
      "Epoch [1400/5000], Loss: 0.0000\n",
      "Epoch [1500/5000], Loss: 0.0000\n",
      "Epoch [1600/5000], Loss: 0.0000\n",
      "Epoch [1700/5000], Loss: 0.0000\n",
      "Epoch [1800/5000], Loss: 0.0000\n",
      "Epoch [1900/5000], Loss: 0.0000\n",
      "Epoch [2000/5000], Loss: 0.0000\n",
      "Epoch [2100/5000], Loss: 0.0000\n",
      "Epoch [2200/5000], Loss: 0.0000\n",
      "Epoch [2300/5000], Loss: 0.0000\n",
      "Epoch [2400/5000], Loss: 0.0000\n",
      "Epoch [2500/5000], Loss: 0.0000\n",
      "Epoch [2600/5000], Loss: 0.0000\n",
      "Epoch [2700/5000], Loss: 0.0000\n",
      "Epoch [2800/5000], Loss: 0.0000\n",
      "Epoch [2900/5000], Loss: 0.0000\n",
      "Epoch [3000/5000], Loss: 0.0000\n",
      "Epoch [3100/5000], Loss: 0.0000\n",
      "Epoch [3200/5000], Loss: 0.0000\n",
      "Epoch [3300/5000], Loss: 0.0000\n",
      "Epoch [3400/5000], Loss: 0.0000\n",
      "Epoch [3500/5000], Loss: 0.0000\n",
      "Epoch [3600/5000], Loss: 0.0000\n",
      "Epoch [3700/5000], Loss: 0.0000\n",
      "Epoch [3800/5000], Loss: 0.0000\n",
      "Epoch [3900/5000], Loss: 0.0000\n",
      "Epoch [4000/5000], Loss: 0.0000\n",
      "Epoch [4100/5000], Loss: 0.0000\n",
      "Epoch [4200/5000], Loss: 0.0000\n",
      "Epoch [4300/5000], Loss: 0.0000\n",
      "Epoch [4400/5000], Loss: 0.0000\n",
      "Epoch [4500/5000], Loss: 0.0000\n",
      "Epoch [4600/5000], Loss: 0.0000\n",
      "Epoch [4700/5000], Loss: 0.0000\n",
      "Epoch [4800/5000], Loss: 0.0000\n",
      "Epoch [4900/5000], Loss: 0.0000\n",
      "Epoch [5000/5000], Loss: 0.0000\n",
      "Final Equation: 0.2366698831319809*x + -0.38166505098342896*2 - 0.5297619700431824*3*x + -1.7771629095077515*4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the set of mathematical operations\n",
    "# In this example, we'll consider addition, subtraction, multiplication, and division\n",
    "operations = {\n",
    "    'add': lambda x, y: x + y,\n",
    "    'sub': lambda x, y: x - y,\n",
    "    'mul': lambda x, y: x * y,\n",
    "    'div': lambda x, y: x / y,\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(num_samples):\n",
    "    x = np.linspace(-5, 5, num_samples)\n",
    "    y = x+1\n",
    "    return x, y\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_data, y_data = generate_synthetic_data(100)\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "# Define a simple symbolic regression model\n",
    "class SymbolicRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymbolicRegressionModel, self).__init__()\n",
    "        # Define the trainable coefficients for the operations\n",
    "        self.coeff_add = nn.Parameter(torch.randn(1))\n",
    "        self.coeff_sub = nn.Parameter(torch.randn(1))\n",
    "        self.coeff_mul = nn.Parameter(torch.randn(1))\n",
    "        self.coeff_div = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the symbolic expression to input data\n",
    "        y_pred = (\n",
    "            self.coeff_add * operations['add'](x, 1) +\n",
    "            self.coeff_sub * operations['sub'](x, 2) +\n",
    "            self.coeff_mul * operations['mul'](x, 3) +\n",
    "            self.coeff_div * operations['div'](x, 4)\n",
    "        )\n",
    "        return y_pred\n",
    "\n",
    "# Create the model and define a loss function\n",
    "num_epochs = 5000\n",
    "\n",
    "model = SymbolicRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(x_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the data\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(x_tensor)\n",
    "\n",
    "# Extract the learned coefficients from the trained model\n",
    "coeff_addition = model.coeff_add.item()\n",
    "coeff_subtraction = model.coeff_sub.item()\n",
    "coeff_multiplication = model.coeff_mul.item()\n",
    "coeff_division = model.coeff_div.item()\n",
    "\n",
    "# Define the final symbolic equation\n",
    "final_equation = f\"{coeff_addition}*x + {coeff_subtraction}*2 - {coeff_multiplication}*3*x + {coeff_division}*4\"\n",
    "\n",
    "print(f\"Final Equation: {final_equation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
